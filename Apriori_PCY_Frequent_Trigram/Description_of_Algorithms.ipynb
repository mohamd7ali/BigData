{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # ***Part A:***\n",
    "\n",
    "\n",
    "# ***A-Priori Algorithm***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **A-Priori Algorithm** is a classical approach for frequent itemset mining, which can be effectively used to find frequent trigram patterns in a dataset. Below is a detailed explanation of the method, including its key features, advantages, and limitations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Algorithm Explanation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Preprocessing**\n",
    "- The input dataset (e.g., cleaned abstracts) is tokenized into words.\n",
    "- Trigrams (sequences of three consecutive words) are generated from the tokenized text.\n",
    "\n",
    "### **Step 2: Candidate Generation**\n",
    "- In the first pass, the algorithm counts individual items (e.g., trigrams) to identify frequent trigrams based on a minimum frequency threshold.\n",
    "- Frequent trigrams are retained as candidates for the next pass.\n",
    "\n",
    "### **Step 3: Frequent Itemset Detection**\n",
    "- In the second pass:\n",
    "  - Only trigrams identified as frequent are considered.\n",
    "  - Further combinations of trigrams are explored to refine the candidate set.\n",
    "- This iterative process continues until no further frequent sets are found.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Advantages of the A-Priori Algorithm**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Memory Efficiency**:\n",
    "   - A-Priori reduces the search space by eliminating infrequent candidates early in the process.\n",
    "   - This pruning mechanism minimizes memory usage.\n",
    "\n",
    "2. **Easy to Implement**:\n",
    "   - The algorithm's steps are straightforward and can be implemented using map-reduce frameworks or basic programming constructs.\n",
    "\n",
    "3. **Widely Applicable**:\n",
    "   - Suitable for small to medium-sized datasets and scenarios where frequent patterns are sparsely distributed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Limitations of the A-Priori Algorithm**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Computational Overhead**:\n",
    "   - Requires multiple passes over the dataset, which increases computation time for large datasets.\n",
    "   \n",
    "2. **Scalability Issues**:\n",
    "   - Not ideal for very large datasets due to its iterative nature.\n",
    "\n",
    "3. **Candidate Explosion**:\n",
    "   - For dense datasets, the number of candidates can grow exponentially, making the algorithm inefficient.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Algorithm Hyperparameters**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Threshold (`threshold`)**\n",
    "   - **Purpose**: Determines the minimum frequency for a trigram to be considered frequent.\n",
    "   - **Value Used**: `threshold = 100`\n",
    "   - **Effect**:\n",
    "     - **Lower Threshold**: Captures more patterns but increases computational cost.\n",
    "     - **Higher Threshold**: Reduces the candidate set size but may miss meaningful patterns.\n",
    "\n",
    "### 2. **Number of Passes**\n",
    "   - A-Priori typically requires multiple passes over the dataset to refine candidate sets iteratively.\n",
    "   - The number of passes is proportional to the depth of frequent itemsets being explored.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Comparison with PCY Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Feature                        | A-Priori                         | PCY                              |\n",
    "|--------------------------------|-----------------------------------|----------------------------------|\n",
    "| **Memory Usage**               | Higher due to explicit counting  | Lower with bitmap compression   |\n",
    "| **Computational Overhead**     | Multiple dataset passes          | Reduced with bucket filtering   |\n",
    "| **Scalability**                | Limited for large datasets       | Scales better for larger data   |\n",
    "| **Ease of Implementation**     | Simple and intuitive             | Slightly more complex           |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **When to Use A-Priori?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Small to Medium Datasets**:\n",
    "   - Suitable for datasets where frequent patterns are sparsely distributed.\n",
    "\n",
    "2. **Exploratory Analysis**:\n",
    "   - Use for understanding basic frequent patterns before optimizing further with advanced algorithms like PCY.\n",
    "\n",
    "3. **Resource-Constrained Systems**:\n",
    "   - When memory optimization is less critical compared to ease of implementation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Summary**\n",
    "\n",
    "The A-Priori Algorithm provides a robust foundation for frequent pattern mining. While its iterative nature and memory requirements pose challenges for large datasets, its simplicity and effectiveness in smaller datasets make it a valuable tool for exploratory data analysis. For larger datasets, more advanced algorithms like PCY can be employed to address scalability concerns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> # ***Part B:***\n",
    "# ***PCY Algorithm***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **PCY Algorithm (Park-Chen-Yu)** is a more memory-efficient version of the **Apriori Algorithm** used to mine frequent itemsets (or trigrams in this case). It utilizes hashing and bitmaps to reduce memory usage and minimize the number of candidates that need to be evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCY Algorithm Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Step 1: Pass 1 - Count Trigrams and Create Buckets**\n",
    "1. Read the dataset and split text into trigrams (e.g., \"large language model\").\n",
    "2. Hash each trigram into buckets using a hash function. The hash function assigns trigrams to specific buckets based on their values.\n",
    "3. Maintain a count of:\n",
    "   - Individual trigram frequencies.\n",
    "   - Frequency of each bucket (the total count of trigrams mapped to that bucket).\n",
    "\n",
    "### **Step 2: Create a Bitmap**\n",
    "- After the first pass, identify \"frequent buckets\" — buckets whose total counts exceed the predefined **frequency threshold**.\n",
    "- Store these frequent buckets in a **bitmap** (a binary representation to indicate frequent/infrequent buckets).\n",
    "\n",
    "### **Step 3: Pass 2 - Filter and Count Frequent Trigrams**\n",
    "1. Revisit the dataset and evaluate each trigram:\n",
    "   - Check if the trigram's hashed bucket is \"frequent\" (as per the bitmap).\n",
    "   - Check if the trigram itself meets the frequency threshold.\n",
    "2. Count only those trigrams that meet both conditions.\n",
    "\n",
    "### **Step 4: Output Results**\n",
    "- Extract the most frequent trigrams along with their frequencies.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences Between PCY and Apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Feature                        | Apriori                                  | PCY                                      |\n",
    "|--------------------------------|------------------------------------------|------------------------------------------|\n",
    "| **Memory Efficiency**          | Requires memory to store all candidate   | Uses hashing to reduce memory usage.     |\n",
    "| **Optimization Technique**     | Prunes candidates based on thresholds.   | Uses a bitmap and hash buckets to prune. |\n",
    "| **Passes Over Dataset**        | Multiple passes for each itemset size.   | Reduces the size of candidates after Pass 1. |\n",
    "| **False Negatives**            | None.                                   | Possible if a frequent trigram hashes to an infrequent bucket. |\n",
    "| **False Positives**            | None.                                   | Bitmap might allow infrequent trigrams if their bucket is frequent. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of PCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Memory Efficiency:**\n",
    "   - By using hash buckets and bitmaps, PCY avoids storing all candidate itemsets, reducing memory usage compared to Apriori.\n",
    "\n",
    "2. **Scalability:**\n",
    "   - PCY can process larger datasets as it reduces the number of candidates for frequent itemset evaluation through its bitmap.\n",
    "\n",
    "3. **Speed:**\n",
    "   - With fewer candidates to process in the second pass, PCY is computationally faster than Apriori for large datasets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of PCY Over Apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Memory Optimization**:\n",
    "   - Uses bitmaps to store bucket information instead of storing all candidates.\n",
    "   - Hashing reduces the number of candidate trigrams significantly.\n",
    "\n",
    "2. **Scalability**:\n",
    "   - Handles larger datasets better due to efficient memory utilization.\n",
    "\n",
    "3. **Reduced Candidates**:\n",
    "   - Eliminates many infrequent trigrams early in the process, reducing computational overhead.\n",
    "\n",
    "4. **Better Performance**:\n",
    "   - Faster execution due to reduced number of candidates in the second pass.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Limitations of PCY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Approximation-Based:**\n",
    "   - PCY approximates the frequent itemsets, potentially introducing false positives and false negatives.\n",
    "\n",
    "2. **Hash Collisions:**\n",
    "   - Frequent hash collisions in the bucket may result in some false positives, affecting the precision.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions of the PCY Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Multi-Stage**\n",
    "\n",
    "2. **Multi-Hashing**:\n",
    "   - Use multiple hash functions and bitmaps to reduce false negatives.\n",
    "\n",
    "2. **Dynamic Bucket Sizing**:\n",
    "   - Dynamically adjust bucket sizes based on dataset characteristics.\n",
    "\n",
    "3. **Second-Pass Optimization**:\n",
    "   - Use smaller hash tables in the second pass for further efficiency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The implementation includes the following extensions to the basic PCY algorithm:\n",
    "\n",
    "### 1. **Dynamic Hash Buckets (Bucket Size)**\n",
    "   - **Description**: Controls the number of buckets used for hashing trigrams.\n",
    "   - **Benefit**: Larger buckets reduce hash collisions, minimizing false negatives.\n",
    "   - **Purpose**: Ensures scalability by allowing flexible bucket sizes based on memory constraints or dataset characteristics.\n",
    "\n",
    "### 2. **Bitmap for Frequent Buckets**\n",
    "   - **Description**: Stores a binary value (frequent/infrequent) for each bucket instead of all bucket counts.\n",
    "   - **Benefit**: Reduces memory overhead significantly and speeds up filtering.\n",
    "\n",
    "### 3. **Second-Pass Filtering**\n",
    "   - **Description**: Evaluates only trigrams that hash to \"frequent\" buckets in the second pass.\n",
    "   - **Benefit**: Reduces computational overhead by ignoring infrequent trigrams early.\n",
    "\n",
    "### 4. **Hashing with MD5**\n",
    "   - **Description**: Uses a robust hash function (`hashlib.md5`) to evenly distribute trigrams across buckets.\n",
    "   - **Benefit**: Minimizes hash collisions, leading to better bucket utilization and fewer false positives.\n",
    "\n",
    "### 5. **Frequency Threshold**\n",
    "   - **Description**: Filters trigrams and buckets based on a minimum frequency threshold.\n",
    "   - **Benefit**: Reduces the candidate set size and focuses on meaningful patterns.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hyperparameters in the Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. **Bucket Size (`bucket_size`)**\n",
    "   - **Purpose**: Determines the number of buckets used in the first pass.\n",
    "   - **Value Used**: `bucket_size = 1000`\n",
    "   - **Alternative Configurations**:\n",
    "     - **Smaller Bucket Size (e.g., 500)**:\n",
    "       - Increases hash collisions, potentially leading to false negatives.\n",
    "     - **Larger Bucket Size (e.g., 5000)**:\n",
    "       - Reduces collisions but requires more memory, suitable for larger datasets.\n",
    "\n",
    "### 2. **Threshold (`threshold`)**\n",
    "   - **Purpose**: Sets the minimum frequency for a trigram or bucket to be considered frequent.\n",
    "   - **Value Used**: `threshold = 100`\n",
    "   - **Alternative Configurations**:\n",
    "     - **Lower Threshold (e.g., 50)**:\n",
    "       - Detects less frequent patterns but increases computation in the second pass.\n",
    "     - **Higher Threshold (e.g., 500)**:\n",
    "       - Focuses on the most frequent patterns, reducing candidates but may miss meaningful patterns.\n",
    "\n",
    "### 3. **Hash Function**\n",
    "   - **Purpose**: Maps trigrams to buckets for efficient counting.\n",
    "   - **Value Used**: `hashlib.md5`\n",
    "   - **Alternative Configurations**:\n",
    "     - **Simpler hash functions (e.g., Python’s `hash()`)**:\n",
    "       - Faster but more prone to uneven distribution.\n",
    "     - **Cryptographic hash functions like SHA-1**:\n",
    "       - Robust but computationally heavier.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Effect of Hyperparameters on Results**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bucket Size**\n",
    "   - **Small Values**:\n",
    "     - Increase hash collisions, leading to more false negatives.\n",
    "   - **Large Values**:\n",
    "     - Reduce collisions but consume more memory.\n",
    "\n",
    "### **Threshold**\n",
    "   - **Low Threshold**:\n",
    "     - Captures less frequent patterns but increases computational overhead.\n",
    "   - **High Threshold**:\n",
    "     - Reduces noise but may miss meaningful infrequent trigrams.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Choosing Optimal Values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Dataset Size**:\n",
    "   - Larger datasets require more buckets (`bucket_size`) to distribute trigrams evenly.\n",
    "   - A higher threshold (`threshold`) is recommended to filter noise.\n",
    "\n",
    "2. **Memory Constraints**:\n",
    "   - For systems with limited memory, prioritize smaller bucket sizes and bitmaps, even if hash collisions increase slightly.\n",
    "\n",
    "3. **Domain-Specific Goals**:\n",
    "   - For exploratory analysis, use a lower threshold to reveal hidden patterns.\n",
    "   - For focused analysis, use higher thresholds to filter noise and identify impactful results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass 1: Count Trigrams and Hash Buckets\n",
    "```python\n",
    "# Hash trigrams into buckets and count their occurrences\n",
    "bucket_counts = trigrams_rdd.map(lambda trigram: (hash_trigram(trigram), 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Identify frequent buckets\n",
    "frequent_buckets = bucket_counts.filter(lambda x: x[1] >= threshold).map(lambda x: x[0]).collect()\n",
    "bitmap = set(frequent_buckets)\n",
    "```\n",
    "\n",
    "### Pass 2: Filter and Count Frequent Trigrams\n",
    "```python\n",
    "# Count trigrams that hash to frequent buckets and meet frequency threshold\n",
    "frequent_trigrams = trigram_counts.filter(lambda x: hash_trigram(x[0]) in bitmap and x[1] >= threshold)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The PCY Algorithm provides a significant improvement over Apriori for frequent trigram mining, especially in terms of memory usage and computational efficiency. However, it introduces potential false positives and negatives due to its reliance on hashing and bitmaps. Extensions like multi-hashing or dynamic bucket sizing can further improve its performance. There is also Trade-offs, Balancing memory usage and computational efficiency is key to achieving optimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> # ***Part C:***\n",
    "# ***Evaluation of PCY Model Using Jaccard***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The Jaccard Index is a widely used metric to evaluate the accuracy of frequent itemset models. It quantifies the similarity between the predicted frequent itemsets (`P`) and the actual frequent itemsets (`A`) as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Jaccard Index** is defined as:\n",
    "\n",
    "$$\n",
    "Jaccard = \\frac{|P \\cap A|}{|P \\cup A|}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- **P**: The set of frequent trigrams predicted by the PCY algorithm.\n",
    "- **A**: The actual set of frequent trigrams (computed using the A-Priori algorithm).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "- **TP (True Positives):** The number of trigrams correctly identified as frequent by both PCY and Apriori.\n",
    "- **FP (False Positives):** The number of trigrams identified as frequent by PCY but not by Apriori.\n",
    "- **FN (False Negatives):** The number of trigrams identified as frequent by Apriori but missed by PCY.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluation Steps - Steps to Compute the Jaccard**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. **Compute Frequent Trigrams Using Both Methods**\n",
    "   - Use the results of the Apriori algorithm as the ground truth (`A`) since it directly scans all candidates without approximations.\n",
    "   - Compare with the results from the PCY algorithm (`P`).\n",
    "\n",
    "   - **PCY Results (P):** Use the results from the PCY algorithm:\n",
    "      ```python\n",
    "      pcy_results = frequent_trigrams.map(lambda x: x[0]).collect()\n",
    "      ```\n",
    "\n",
    "   - **A-Priori Results (A):** Use the results from the A-Priori algorithm:\n",
    "      ```python\n",
    "      apriori_results = frequent_trigrams_rdd.map(lambda x: x[0]).collect()\n",
    "      ```\n",
    "\n",
    "### 2. **Intersection and Union Calculation**\n",
    "   - Compute the intersection |P ∩ A|: Trigrams present in both PCY and Apriori results.\n",
    "\n",
    "   - Compute the union |P ∪ A|: All unique trigrams from PCY and Apriori results combined.\n",
    "\n",
    "   - Compute the intersection of **P** and **A**:\n",
    "      ```python\n",
    "      true_positives = set(pcy_results).intersection(set(apriori_results))\n",
    "      ```\n",
    "\n",
    "   - Compute the union of **P** and **A**:\n",
    "      ```python\n",
    "      union = set(pcy_results).union(set(apriori_results))\n",
    "      ```\n",
    "\n",
    "### 3. **Jaccard Calculation**\n",
    "   - Calculate the Jaccard using the formula:\n",
    "     $$\n",
    "     Jaccard = \\frac{|P \\cap A|}{|P \\cup A|}\n",
    "     $$\n",
    "\n",
    "   - Use the formula to compute the Jaccard:\n",
    "      ```python\n",
    "      jaccard_index = len(true_positives) / len(union)\n",
    "      print(f\"Jaccard: {jaccard_index:.4f}\")\n",
    "      ```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Advantages of Jaccard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Robust Metric:** Provides a reliable measure of overlap between predicted and actual results.\n",
    "- **False Positive and Negative Sensitivity:** Takes into account both false positives and false negatives.\n",
    "- **Interpretability:** A value close to 1 indicates high agreement, while a value close to 0 indicates low agreement.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Observations from Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- A **high Jaccard Index** indicates strong agreement between PCY and Apriori, showcasing PCY’s ability to capture frequent trigrams effectively.\n",
    "- Variations in **hyperparameters** such as bucket size and threshold influence the Jaccard Index:\n",
    "  - **Large Bucket Size:** Reduces hash collisions, improving the Jaccard score.\n",
    "  - **High Threshold:** Filters noise but may miss less frequent patterns.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
